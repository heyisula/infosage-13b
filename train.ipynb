{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header_md",
            "metadata": {},
            "source": [
                "# Production QLoRA Training — Llama-2-13B (H100 Optimized)\n",
                "\n",
                "Enterprise-grade fine-tuning pipeline optimized for NVIDIA H100 80GB.\n",
                "\n",
                "**Key Optimizations for Stability**:\n",
                "- **Gradient Checkpointing**: CRITICAL fix to reduce VRAM from >80GB to ~25GB.\n",
                "- **Batch Size 2 / Accum 4**: Effective batch 8, but minimizing peak memory.\n",
                "- **8-bit Optimizer**: Reduces fragmentation.\n",
                "- **Memory Callback**: Aggressive cache clearing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install_deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U bitsandbytes accelerate peft transformers datasets tqdm faiss-cpu sentence-transformers flash-attn --no-build-isolation\n",
                "\n",
                "try:\n",
                "    import bitsandbytes\n",
                "    print(f'[OK] bitsandbytes {bitsandbytes.__version__}')\n",
                "except ImportError:\n",
                "    print('[FATAL] bitsandbytes not found. Restarting runtime...')\n",
                "    import os; os.kill(os.getpid(), 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mount_drive",
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "hw_diag",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import time\n",
                "\n",
                "# === Hardware Verification ===\n",
                "assert torch.cuda.is_available(), 'FATAL: No CUDA device found'\n",
                "\n",
                "gpu_name = torch.cuda.get_device_name(0)\n",
                "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "\n",
                "print('=' * 60)\n",
                "print('HARDWARE DIAGNOSTICS')\n",
                "print('=' * 60)\n",
                "print(f'  GPU:           {gpu_name}')\n",
                "print(f'  VRAM:          {vram_gb:.1f} GB')\n",
                "print(f'  CUDA Version:  {torch.version.cuda}')\n",
                "print(f'  PyTorch:       {torch.__version__}')\n",
                "print(f'  BF16 Support:  {torch.cuda.is_bf16_supported()}')\n",
                "\n",
                "# Enable TF32 for H100/A100/RTX6000 tensor core acceleration (PyTorch 2.9+ style)\n",
                "torch.backends.cuda.matmul.allow_tf32 = True # Fallback\n",
                "try:\n",
                "    torch.backends.cuda.matmul.fp32_precision = 'tf32'\n",
                "    torch.backends.cuda.conv.fp32_precision = 'tf32'\n",
                "    print(f'  TF32:          Enabled (Strict)')\n",
                "except AttributeError:\n",
                "    print(f'  TF32:          Enabled (Legacy)')\n",
                "\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_md",
            "metadata": {},
            "source": [
                "## 1. Load Model — 4-bit QLoRA + Flash Attention 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    DataCollatorForLanguageModeling,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "from tqdm import tqdm\n",
                "\n",
                "MODEL_NAME = 'NousResearch/Llama-2-13b-hf'\n",
                "MAX_LENGTH = 1024 # Reduced from 2048 to save memory\n",
                "\n",
                "print(f'Loading {MODEL_NAME} in 4-bit NF4...')\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.model_max_length = MAX_LENGTH\n",
                "# Suppress warning about padding, since we handle it dynamically\n",
                "tokenizer.deprecation_warnings = {'Asking-to-pad-to-max_length': True}\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type='nf4',\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map='auto',\n",
                "    attn_implementation='flash_attention_2'\n",
                ")\n",
                "\n",
                "# CRITICAL: Enable Gradient Checkpointing to save ~40GB VRAM\n",
                "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
                "model.gradient_checkpointing_enable()\n",
                "\n",
                "# Freeze all base model parameters\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "print('Base model loaded. Applying LoRA...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "apply_lora",
            "metadata": {},
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=32,\n",
                "    lora_alpha=64,\n",
                "    lora_dropout=0.05,\n",
                "    bias='none',\n",
                "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "\n",
                "# Verify only LoRA params are trainable\n",
                "trainable, total = 0, 0\n",
                "for p in model.parameters():\n",
                "    total += p.numel()\n",
                "    if p.requires_grad:\n",
                "        trainable += p.numel()\n",
                "\n",
                "print('=' * 60)\n",
                "print('LORA CONFIGURATION')\n",
                "print('=' * 60)\n",
                "print(f'  Total params:     {total:,}')\n",
                "print(f'  Trainable params: {trainable:,}')\n",
                "print(f'  Trainable %:      {100 * trainable / total:.2f}%')\n",
                "print(f'  LoRA rank:        32')\n",
                "print(f'  LoRA alpha:       64')\n",
                "print(f'  Targets:          q_proj, k_proj, v_proj, o_proj')\n",
                "print(f'  Sequence length:  {MAX_LENGTH}')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_md",
            "metadata": {},
            "source": [
                "## 2. Prepare Streaming Data (Robust Cleaning)\n",
                "\n",
                "**Strategy:**\n",
                "- **Streaming**: Never load full dataset into RAM.\n",
                "- **Dynamic Padding**: Tokenize with `padding=False`.\n",
                "- **Clean Columns**: Auto-detects and removes non-tensor columns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "print('Initializing dataset stream...')\n",
                "\n",
                "# Streaming mode: Load on demand\n",
                "raw_dataset = load_dataset(\n",
                "    'HuggingFaceFW/fineweb-edu',\n",
                "    split='train',\n",
                "    streaming=True\n",
                ")\n",
                "\n",
                "# Shuffle buffer for randomness\n",
                "shuffled_dataset = raw_dataset.shuffle(seed=42, buffer_size=10000)\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    # Dynamic padding: NO padding here, just truncation\n",
                "    return tokenizer(\n",
                "        examples['text'],\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=False # CRITICAL: Dynamic padding handled by collator\n",
                "    )\n",
                "\n",
                "print('Configuring tokenization stream...')\n",
                "\n",
                "# Inspect first sample to find all columns that need removal\n",
                "try:\n",
                "    sample = next(iter(shuffled_dataset))\n",
                "    remove_cols = list(sample.keys())\n",
                "    print(f'Detected columns to remove: {remove_cols}')\n",
                "except Exception as e:\n",
                "    print(f'Warning: Could not auto-detect columns ({e}). Using default list.')\n",
                "    remove_cols = ['text', 'id', 'url', 'file_name', 'timestamp', 'dump', 'segment', 'token_count']\n",
                "\n",
                "tokenized_dataset = shuffled_dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    remove_columns=remove_cols\n",
                ")\n",
                "\n",
                "print('Dataset stream ready.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "train_md",
            "metadata": {},
            "source": [
                "## 3. Training Configuration (Memory Optimized)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = '/content/drive/MyDrive/fineweb_edu_llama2_13b/checkpoints'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=output_dir,\n",
                "    per_device_train_batch_size=2,   # Reduced from 8 to 2\n",
                "    gradient_accumulation_steps=4,   # Increased from 1 to 4 (Net 8)\n",
                "    learning_rate=1e-4,\n",
                "    lr_scheduler_type='cosine',\n",
                "    warmup_steps=150,                # Fixed steps instead of ratio\n",
                "    max_steps=5000,\n",
                "    bf16=True,\n",
                "    fp16=False,\n",
                "    optim='adamw_bnb_8bit',          # 8-bit optimizer to save memory\n",
                "    gradient_checkpointing=True,     # CRITICAL for memory savings\n",
                "    logging_steps=20,\n",
                "    save_steps=500,\n",
                "    save_total_limit=3,\n",
                "    report_to='none',\n",
                "    remove_unused_columns=True,\n",
                "    dataloader_num_workers=4,        # Reduced CPU memory pressure\n",
                "    dataloader_pin_memory=True,\n",
                "    dataloader_drop_last=True,\n",
                "    dataloader_persistent_workers=True\n",
                ")\n",
                "\n",
                "print('=' * 60)\n",
                "print('TRAINING ARGUMENTS')\n",
                "print('=' * 60)\n",
                "print(f'  Batch Size:          2 (per device)')\n",
                "print(f'  Grad Accumulation:   4')\n",
                "print(f'  Effective Batch:     8')\n",
                "print(f'  Max Steps:           5,000')\n",
                "print(f'  Precision:           BF16')\n",
                "print(f'  Grad Checkpointing:  ENABLED (Critical)')\n",
                "print(f'  Optimizer:           adamw_bnb_8bit')\n",
                "print(f'  LR:                  1e-4 (cosine, 150 warmup steps)')\n",
                "print(f'  Dataloader Workers:  4')\n",
                "print(f'  Padding:             Dynamic')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_trainer",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "from transformers import TrainerCallback\n",
                "import gc\n",
                "\n",
                "class MemoryCallback(TrainerCallback):\n",
                "    \"\"\"Aggressively cleans memory to prevent fragmentation.\"\"\"\n",
                "    def on_step_end(self, args, state, control, **kwargs):\n",
                "        torch.cuda.empty_cache()\n",
                "        gc.collect()\n",
                "\n",
                "class ThroughputCallback(TrainerCallback):\n",
                "    \"\"\"Logs throughput and warns if below target.\"\"\"\n",
                "    def __init__(self, batch_size, grad_accum):\n",
                "        self.batch_size = batch_size * grad_accum\n",
                "        self.start_time = None\n",
                "        self.start_step = 0\n",
                "\n",
                "    def on_step_begin(self, args, state, control, **kwargs):\n",
                "        if self.start_time is None:\n",
                "            self.start_time = time.time()\n",
                "            self.start_step = state.global_step\n",
                "\n",
                "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
                "        if self.start_time is None or state.global_step <= self.start_step:\n",
                "            return\n",
                "        elapsed = time.time() - self.start_time\n",
                "        steps_done = state.global_step - self.start_step\n",
                "        if steps_done == 0:\n",
                "            return\n",
                "\n",
                "        steps_per_sec = steps_done / elapsed\n",
                "        remaining_steps = args.max_steps - state.global_step\n",
                "        eta_min = remaining_steps / steps_per_sec / 60 if steps_per_sec > 0 else float('inf')\n",
                "\n",
                "        print(f'  [PERF] Step {state.global_step}/{args.max_steps} | '\n",
                "              f'{steps_per_sec:.2f} it/s | '\n",
                "              f'ETA: {eta_min:.0f} min')\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator,\n",
                "    callbacks=[\n",
                "        ThroughputCallback(batch_size=2, grad_accum=4),\n",
                "        MemoryCallback()\n",
                "    ]\n",
                ")\n",
                "\n",
                "print('Trainer ready.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "run_md",
            "metadata": {},
            "source": [
                "## 4. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_training",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers.trainer_utils import get_last_checkpoint\n",
                "\n",
                "last_checkpoint = get_last_checkpoint(output_dir)\n",
                "\n",
                "train_start = time.time()\n",
                "\n",
                "try:\n",
                "    if last_checkpoint is not None:\n",
                "        print(f'Resuming from checkpoint: {last_checkpoint}')\n",
                "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
                "    else:\n",
                "        print('Starting fresh Llama-2-13B QLoRA training...')\n",
                "        trainer.train()\n",
                "\n",
                "    train_elapsed = time.time() - train_start\n",
                "    print('=' * 60)\n",
                "    print(f'TRAINING COMPLETE — {train_elapsed / 60:.1f} minutes')\n",
                "    print('=' * 60)\n",
                "\n",
                "except torch.cuda.OutOfMemoryError:\n",
                "    print('\\n' + '=' * 60)\n",
                "    print('FATAL: CUDA Out of Memory')\n",
                "    print('=' * 60)\n",
                "    print(f'  Allocated: {torch.cuda.memory_allocated() / 1024**3:.1f} GB')\n",
                "    print(f'  Reserved:  {torch.cuda.memory_reserved() / 1024**3:.1f} GB')\n",
                "    print(f'  Total:     {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
                "    print('=' * 60)\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "final_model_dir = '/content/drive/MyDrive/fineweb_edu_llama2_13b/final_model'\n",
                "print(f'Saving LoRA adapters to: {final_model_dir}')\n",
                "trainer.save_model(final_model_dir)\n",
                "tokenizer.save_pretrained(final_model_dir)\n",
                "print('Model saved successfully.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag_md",
            "metadata": {},
            "source": [
                "## 5. Build RAG Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_build",
            "metadata": {},
            "outputs": [],
            "source": [
                "import faiss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np\n",
                "\n",
                "RAG_SAMPLES = 100_000\n",
                "RAG_DIR = '/content/drive/MyDrive/fineweb_edu_llama2_13b/rag_index'\n",
                "os.makedirs(RAG_DIR, exist_ok=True)\n",
                "\n",
                "# Reload streaming to get raw text for RAG\n",
                "rag_stream = raw_dataset.take(RAG_SAMPLES)\n",
                "\n",
                "passages = []\n",
                "print('Extracting passages...')\n",
                "for row in tqdm(rag_stream, total=RAG_SAMPLES):\n",
                "    text = row['text'].strip()\n",
                "    for i in range(0, len(text), 500):\n",
                "        chunk = text[i:i + 500].strip()\n",
                "        if len(chunk) > 50:\n",
                "            passages.append(chunk)\n",
                "\n",
                "print(f'Encoding {len(passages):,} passages...')\n",
                "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                "embeddings = embedder.encode(passages, show_progress_bar=True, batch_size=256, convert_to_numpy=True)\n",
                "\n",
                "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
                "faiss.normalize_L2(embeddings)\n",
                "index.add(embeddings)\n",
                "\n",
                "faiss.write_index(index, os.path.join(RAG_DIR, 'faiss_index.bin'))\n",
                "np.save(os.path.join(RAG_DIR, 'passages.npy'), np.array(passages, dtype=object))\n",
                "print(f'RAG index saved: {len(passages):,} passages, dim={embeddings.shape[1]}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}