{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Fine-Tune GPT-2 Medium on FineWeb-Edu\n",
                "\n",
                "This notebook fine-tunes the **GPT-2 Medium (355M params)** pre-trained model on high-quality educational content from FineWeb-Edu.\n",
                "\n",
                "**Hardware**: RTX 4060 (8GB VRAM) — uses gradient checkpointing + Adafactor + fp16 to fit."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84a6e95e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset, Dataset\n",
                "from transformers import (\n",
                "    GPT2LMHeadModel,\n",
                "    GPT2TokenizerFast,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    DataCollatorForLanguageModeling\n",
                ")\n",
                "import os\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c5c5849",
            "metadata": {},
            "outputs": [],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5f6g7h8",
            "metadata": {},
            "source": [
                "## 1. Load Pre-trained GPT-2 Medium Model & Tokenizer\n",
                "\n",
                "Instead of training from scratch, we start with the pre-trained GPT-2 Medium (355M params) and fine-tune it.\n",
                "This gives us a strong language foundation — the model already understands English, grammar, and general knowledge."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2307b3a",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"gpt2-medium\"\n",
                "\n",
                "print(f\"Loading pre-trained model: {MODEL_NAME}\")\n",
                "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
                "\n",
                "# Enable gradient checkpointing to fit in 8GB VRAM\n",
                "model.gradient_checkpointing_enable()\n",
                "\n",
                "num_params = sum(p.numel() for p in model.parameters())\n",
                "print(f\"Model loaded: {num_params / 1e6:.1f}M parameters\")\n",
                "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
                "print(f\"Max context length: {model.config.n_positions}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "i9j0k1l2",
            "metadata": {},
            "source": [
                "## 2. Load & Materialize FineWeb-Edu Dataset\n",
                "\n",
                "We use 500K samples from the FineWeb-Edu dataset — high-quality educational web content curated by HuggingFace."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37a18622",
            "metadata": {},
            "outputs": [],
            "source": [
                "NUM_SAMPLES = 500_000\n",
                "\n",
                "print(f\"Loading {NUM_SAMPLES:,} samples from FineWeb-Edu (streaming)...\")\n",
                "dataset = load_dataset(\n",
                "    \"HuggingFaceFW/fineweb-edu\",\n",
                "    split=\"train\",\n",
                "    streaming=True\n",
                ")\n",
                "\n",
                "# Materialize the streaming dataset\n",
                "subset_iter = dataset.take(NUM_SAMPLES)\n",
                "data_list = [row for row in tqdm(subset_iter, total=NUM_SAMPLES, desc=\"Materializing dataset\")]\n",
                "print(f\"Total samples: {len(data_list):,}\")\n",
                "print(f\"Sample keys: {data_list[0].keys()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "m3n4o5p6",
            "metadata": {},
            "source": [
                "## 3. Tokenize the Dataset\n",
                "\n",
                "We tokenize with a context length of 1024 tokens (GPT-2 Medium's full capacity)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3e1ee000",
            "metadata": {},
            "outputs": [],
            "source": [
                "MAX_LENGTH = 1024\n",
                "\n",
                "def tokenize_function(examples):\n",
                "    return tokenizer(\n",
                "        examples[\"text\"],\n",
                "        truncation=True,\n",
                "        padding=\"max_length\",\n",
                "        max_length=MAX_LENGTH\n",
                "    )\n",
                "\n",
                "# Convert to HuggingFace Dataset and tokenize in batches\n",
                "print(\"Converting to HuggingFace Dataset...\")\n",
                "hf_dataset = Dataset.from_list(data_list)\n",
                "\n",
                "print(\"Tokenizing...\")\n",
                "tokenized_dataset = hf_dataset.map(\n",
                "    tokenize_function,\n",
                "    batched=True,\n",
                "    batch_size=1000,\n",
                "    remove_columns=hf_dataset.column_names,\n",
                "    desc=\"Tokenizing\"\n",
                ")\n",
                "\n",
                "print(f\"Tokenized dataset: {len(tokenized_dataset):,} samples\")\n",
                "print(f\"Token sequence length: {MAX_LENGTH}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q7r8s9t0",
            "metadata": {},
            "source": [
                "## 4. Configure Training\n",
                "\n",
                "Optimized for RTX 4060 8GB VRAM:\n",
                "- **Batch size 1** + gradient accumulation 16 = effective batch of 16\n",
                "- **Gradient checkpointing** trades compute for memory (~40% VRAM savings)\n",
                "- **Adafactor optimizer** uses less memory than AdamW\n",
                "- **FP16** mixed precision for speed + memory savings"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "360e73d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = \"out/models/gpt2_medium_finetuned\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "# Data collator handles creating labels (shifted input_ids) automatically\n",
                "data_collator = DataCollatorForLanguageModeling(\n",
                "    tokenizer=tokenizer,\n",
                "    mlm=False  # causal LM, not masked LM\n",
                ")\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=output_dir,\n",
                "    \n",
                "    # Batch size — optimized for 8GB VRAM\n",
                "    per_device_train_batch_size=1,\n",
                "    gradient_accumulation_steps=16,  # effective batch size = 16\n",
                "    \n",
                "    # Learning rate — lower for fine-tuning (not training from scratch)\n",
                "    learning_rate=2e-5,\n",
                "    warmup_steps=500,\n",
                "    weight_decay=0.01,\n",
                "    \n",
                "    # Training duration\n",
                "    num_train_epochs=1,\n",
                "    \n",
                "    # Memory optimization\n",
                "    fp16=True,\n",
                "    gradient_checkpointing=True,\n",
                "    optim=\"adafactor\",\n",
                "    \n",
                "    # Logging & saving\n",
                "    logging_steps=100,\n",
                "    save_steps=2000,\n",
                "    save_total_limit=3,\n",
                "    report_to=\"none\",\n",
                "    \n",
                "    # Dataloader\n",
                "    dataloader_num_workers=2,\n",
                "    dataloader_pin_memory=True,\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_dataset,\n",
                "    data_collator=data_collator,\n",
                ")\n",
                "\n",
                "print(f\"Training config:\")\n",
                "print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
                "print(f\"  Total steps: {len(tokenized_dataset) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
                "print(f\"  Optimizer: {training_args.optim}\")\n",
                "print(f\"  FP16: {training_args.fp16}\")\n",
                "print(f\"  Gradient checkpointing: {training_args.gradient_checkpointing}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "u1v2w3x4",
            "metadata": {},
            "source": [
                "## 5. Train"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28100396",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting fine-tuning...\")\n",
                "print(\"This will take several hours on RTX 4060. Progress is logged every 100 steps.\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "y5z6a7b8",
            "metadata": {},
            "source": [
                "## 6. Save Model & Tokenizer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4fca0420",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Saving fine-tuned model and tokenizer...\")\n",
                "trainer.save_model(output_dir)\n",
                "tokenizer.save_pretrained(output_dir)\n",
                "print(f\"Model saved to: {output_dir}\")\n",
                "print(\"Fine-tuning complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9d0e1f2",
            "metadata": {},
            "source": [
                "## 7. Build RAG Knowledge Base\n",
                "\n",
                "Index a portion of the dataset into a FAISS vector store so the chatbot can retrieve relevant passages in real-time.\n",
                "We use `sentence-transformers/all-MiniLM-L6-v2` for fast, lightweight embeddings."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_build_01",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "try:\n",
                "    import faiss\n",
                "    from sentence_transformers import SentenceTransformer\n",
                "    HAS_RAG_DEPS = True\n",
                "except ImportError:\n",
                "    HAS_RAG_DEPS = False\n",
                "    print(\"RAG dependencies not installed. Run:\")\n",
                "    print(\"  pip install faiss-cpu sentence-transformers\")\n",
                "    print(\"Then re-run this cell.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_build_02",
            "metadata": {},
            "outputs": [],
            "source": [
                "if HAS_RAG_DEPS:\n",
                "    RAG_SAMPLES = 100_000  # number of passages to index\n",
                "    MAX_PASSAGE_LEN = 500  # characters per passage\n",
                "    RAG_DIR = \"out/rag_index\"\n",
                "    os.makedirs(RAG_DIR, exist_ok=True)\n",
                "\n",
                "    print(f\"Building RAG index from {RAG_SAMPLES:,} samples...\")\n",
                "\n",
                "    # Extract and chunk passages\n",
                "    passages = []\n",
                "    for row in tqdm(data_list[:RAG_SAMPLES], desc=\"Extracting passages\"):\n",
                "        text = row[\"text\"].strip()\n",
                "        # Split long documents into chunks\n",
                "        for i in range(0, len(text), MAX_PASSAGE_LEN):\n",
                "            chunk = text[i:i + MAX_PASSAGE_LEN].strip()\n",
                "            if len(chunk) > 50:  # skip tiny fragments\n",
                "                passages.append(chunk)\n",
                "\n",
                "    print(f\"Total passages: {len(passages):,}\")\n",
                "\n",
                "    # Embed passages\n",
                "    print(\"Loading sentence-transformer model...\")\n",
                "    embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "    print(\"Encoding passages (this may take a few minutes)...\")\n",
                "    embeddings = embedder.encode(\n",
                "        passages,\n",
                "        show_progress_bar=True,\n",
                "        batch_size=256,\n",
                "        convert_to_numpy=True\n",
                "    )\n",
                "\n",
                "    # Build FAISS index\n",
                "    print(\"Building FAISS index...\")\n",
                "    dimension = embeddings.shape[1]\n",
                "    index = faiss.IndexFlatIP(dimension)  # Inner Product (cosine similarity on normalized vectors)\n",
                "    faiss.normalize_L2(embeddings)  # normalize for cosine similarity\n",
                "    index.add(embeddings)\n",
                "\n",
                "    # Save index and passages\n",
                "    faiss.write_index(index, os.path.join(RAG_DIR, \"faiss_index.bin\"))\n",
                "    np.save(os.path.join(RAG_DIR, \"passages.npy\"), np.array(passages, dtype=object))\n",
                "\n",
                "    print(f\"RAG index saved to {RAG_DIR}/\")\n",
                "    print(f\"  Index: {index.ntotal:,} vectors, {dimension}D\")\n",
                "    print(f\"  Passages: {len(passages):,}\")\n",
                "    print(\"Done!\")\n",
                "else:\n",
                "    print(\"Skipping RAG index build (dependencies not installed).\")\n",
                "    print(\"You can build it later by running: python build_rag_index.py\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}