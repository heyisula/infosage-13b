{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header_md",
            "metadata": {},
            "source": [
                "# Production QLoRA Training — Llama-2-13B on H100\n",
                "\n",
                "Enterprise-grade fine-tuning pipeline optimized for NVIDIA H100 80GB HBM3.\n",
                "\n",
                "**Target**: 5,000 steps in ~45–60 minutes at ~1.2–1.6 it/s."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "env_setup",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
                "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
                "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "install_deps",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q -U bitsandbytes accelerate peft transformers datasets tqdm faiss-cpu sentence-transformers flash-attn --no-build-isolation\n",
                "\n",
                "try:\n",
                "    import bitsandbytes\n",
                "    print(f'[OK] bitsandbytes {bitsandbytes.__version__}')\n",
                "except ImportError:\n",
                "    print('[FATAL] bitsandbytes not found. Restarting runtime...')\n",
                "    import os; os.kill(os.getpid(), 9)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "mount_drive",
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "diag_md",
            "metadata": {},
            "source": [
                "## 0. Hardware Diagnostics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "hw_diag",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import time\n",
                "\n",
                "# === H100 Hardware Verification ===\n",
                "assert torch.cuda.is_available(), 'FATAL: No CUDA device found'\n",
                "\n",
                "gpu_name = torch.cuda.get_device_name(0)\n",
                "vram_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
                "\n",
                "print('=' * 60)\n",
                "print('HARDWARE DIAGNOSTICS')\n",
                "print('=' * 60)\n",
                "print(f'  GPU:           {gpu_name}')\n",
                "print(f'  VRAM:          {vram_gb:.1f} GB')\n",
                "print(f'  CUDA Version:  {torch.version.cuda}')\n",
                "print(f'  PyTorch:       {torch.__version__}')\n",
                "print(f'  BF16 Support:  {torch.cuda.is_bf16_supported()}')\n",
                "\n",
                "# Enable TF32 for H100 tensor core acceleration\n",
                "torch.backends.cuda.matmul.allow_tf32 = True\n",
                "torch.backends.cudnn.allow_tf32 = True\n",
                "print(f'  TF32:          Enabled')\n",
                "\n",
                "if vram_gb < 70:\n",
                "    print(f'  ⚠ WARNING: This notebook is optimized for H100 (80GB). Current GPU has {vram_gb:.0f} GB.')\n",
                "else:\n",
                "    print(f'  ✓ H100 detected. Full speed mode.')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_md",
            "metadata": {},
            "source": [
                "## 1. Load Model — 4-bit QLoRA + Flash Attention 2\n",
                "\n",
                "- **No gradient checkpointing** (H100 has headroom)\n",
                "- **Flash Attention 2** for O(N) memory attention\n",
                "- **BF16 compute dtype** for H100 tensor cores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import (\n",
                "    AutoModelForCausalLM,\n",
                "    AutoTokenizer,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    DataCollatorForLanguageModeling,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "from tqdm import tqdm\n",
                "\n",
                "MODEL_NAME = 'NousResearch/Llama-2-13b-hf'\n",
                "MAX_LENGTH = 2048\n",
                "\n",
                "print(f'Loading {MODEL_NAME} in 4-bit NF4...')\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.model_max_length = MAX_LENGTH\n",
                "\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type='nf4',\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    quantization_config=bnb_config,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map='auto',\n",
                "    attn_implementation='flash_attention_2'\n",
                ")\n",
                "\n",
                "# Prepare for QLoRA — explicitly disable gradient checkpointing\n",
                "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
                "\n",
                "# Freeze all base model parameters\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "print('Base model loaded. Applying LoRA...')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "apply_lora",
            "metadata": {},
            "outputs": [],
            "source": [
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM,\n",
                "    inference_mode=False,\n",
                "    r=32,\n",
                "    lora_alpha=64,\n",
                "    lora_dropout=0.05,\n",
                "    bias='none',\n",
                "    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj']\n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "\n",
                "# Verify only LoRA params are trainable\n",
                "trainable, total = 0, 0\n",
                "for p in model.parameters():\n",
                "    total += p.numel()\n",
                "    if p.requires_grad:\n",
                "        trainable += p.numel()\n",
                "\n",
                "print('=' * 60)\n",
                "print('LORA CONFIGURATION')\n",
                "print('=' * 60)\n",
                "print(f'  Total params:     {total:,}')\n",
                "print(f'  Trainable params: {trainable:,}')\n",
                "print(f'  Trainable %:      {100 * trainable / total:.2f}%')\n",
                "print(f'  LoRA rank:        32')\n",
                "print(f'  LoRA alpha:       64')\n",
                "print(f'  Targets:          q_proj, k_proj, v_proj, o_proj')\n",
                "print(f'  Sequence length:  {MAX_LENGTH}')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "data_md",
            "metadata": {},
            "source": [
                "## 2. Prepare Streaming Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "NUM_SAMPLES = 1_000_000\n",
                "\n",
                "print(f'Configuring stream for {NUM_SAMPLES:,} samples...')\n",
                "\n",
                "raw_dataset = load_dataset(\n",
                "    'HuggingFaceFW/fineweb-edu',\n",
                "    split='train',\n",
                "    streaming=True\n",
                ")\n",
                "\n",
                "def tokenize_stream(examples):\n",
                "    tokenized = tokenizer(\n",
                "        examples['text'],\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding='max_length'\n",
                "    )\n",
                "    return {\n",
                "        'input_ids': tokenized['input_ids'],\n",
                "        'attention_mask': tokenized['attention_mask']\n",
                "    }\n",
                "\n",
                "sample = next(iter(raw_dataset))\n",
                "all_columns = list(sample.keys())\n",
                "\n",
                "tokenized_dataset = raw_dataset.map(\n",
                "    tokenize_stream,\n",
                "    batched=True,\n",
                "    remove_columns=all_columns,\n",
                "    batch_size=1000\n",
                ")\n",
                "\n",
                "shuffled_dataset = tokenized_dataset.shuffle(seed=42, buffer_size=10_000).take(NUM_SAMPLES)\n",
                "print('Dataset stream ready.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "train_md",
            "metadata": {},
            "source": [
                "## 3. Training Configuration — H100 Production\n",
                "\n",
                "| Parameter | Value |\n",
                "| --- | --- |\n",
                "| Batch Size | 16 |\n",
                "| Grad Accumulation | 1 (none) |\n",
                "| Grad Checkpointing | OFF |\n",
                "| Precision | BF16 |\n",
                "| Optimizer | paged_adamw_32bit |\n",
                "| LR | 1e-4 (cosine, 3% warmup) |\n",
                "| Workers | 8 + pin_memory + persistent |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "train_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = '/content/drive/MyDrive/fineweb_edu_llama2_13b/checkpoints'\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=output_dir,\n",
                "    per_device_train_batch_size=16,\n",
                "    gradient_accumulation_steps=1,\n",
                "    learning_rate=1e-4,\n",
                "    lr_scheduler_type='cosine',\n",
                "    warmup_ratio=0.03,\n",
                "    max_steps=5000,\n",
                "    bf16=True,\n",
                "    fp16=False,\n",
                "    optim='paged_adamw_32bit',\n",
                "    logging_steps=20,\n",
                "    save_steps=500,\n",
                "    save_total_limit=3,\n",
                "    report_to='none',\n",
                "    remove_unused_columns=False,\n",
                "    dataloader_num_workers=8,\n",
                "    dataloader_pin_memory=True,\n",
                "    dataloader_drop_last=True,\n",
                "    dataloader_persistent_workers=True\n",
                ")\n",
                "\n",
                "print('=' * 60)\n",
                "print('TRAINING ARGUMENTS')\n",
                "print('=' * 60)\n",
                "print(f'  Batch Size:          16')\n",
                "print(f'  Grad Accumulation:   1')\n",
                "print(f'  Max Steps:           5,000')\n",
                "print(f'  Precision:           BF16')\n",
                "print(f'  Grad Checkpointing:  OFF')\n",
                "print(f'  Optimizer:           paged_adamw_32bit')\n",
                "print(f'  LR:                  1e-4 (cosine, 3% warmup)')\n",
                "print(f'  Dataloader Workers:  8 (persistent, pinned)')\n",
                "print(f'  TF32:                Enabled')\n",
                "print(f'  Checkpoints:         {output_dir}')\n",
                "print('=' * 60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "build_trainer",
            "metadata": {},
            "outputs": [],
            "source": [
                "import logging\n",
                "from transformers import TrainerCallback\n",
                "\n",
                "class ThroughputCallback(TrainerCallback):\n",
                "    \"\"\"Logs throughput and warns if below target.\"\"\"\n",
                "    def __init__(self, max_length, batch_size, log_interval=100):\n",
                "        self.max_length = max_length\n",
                "        self.batch_size = batch_size\n",
                "        self.log_interval = log_interval\n",
                "        self.start_time = None\n",
                "        self.start_step = 0\n",
                "\n",
                "    def on_step_begin(self, args, state, control, **kwargs):\n",
                "        if self.start_time is None:\n",
                "            self.start_time = time.time()\n",
                "            self.start_step = state.global_step\n",
                "\n",
                "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
                "        if self.start_time is None or state.global_step <= self.start_step:\n",
                "            return\n",
                "        elapsed = time.time() - self.start_time\n",
                "        steps_done = state.global_step - self.start_step\n",
                "        if steps_done == 0:\n",
                "            return\n",
                "\n",
                "        steps_per_sec = steps_done / elapsed\n",
                "        tokens_per_sec = steps_per_sec * self.batch_size * self.max_length\n",
                "        remaining_steps = args.max_steps - state.global_step\n",
                "        eta_min = remaining_steps / steps_per_sec / 60 if steps_per_sec > 0 else float('inf')\n",
                "\n",
                "        print(f'  [PERF] Step {state.global_step}/{args.max_steps} | '\n",
                "              f'{steps_per_sec:.2f} it/s | '\n",
                "              f'{tokens_per_sec:,.0f} tok/s | '\n",
                "              f'ETA: {eta_min:.0f} min')\n",
                "\n",
                "        if steps_per_sec < 1.0 and steps_done > 50:\n",
                "            print(f'  ⚠ WARNING: Throughput {steps_per_sec:.2f} it/s is below target (1.2 it/s). Check config.')\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=shuffled_dataset,\n",
                "    data_collator=data_collator,\n",
                "    callbacks=[ThroughputCallback(MAX_LENGTH, batch_size=16)]\n",
                ")\n",
                "\n",
                "print('Trainer ready.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "run_md",
            "metadata": {},
            "source": [
                "## 4. Train (Auto-Resume + OOM Safety)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "run_training",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers.trainer_utils import get_last_checkpoint\n",
                "\n",
                "last_checkpoint = get_last_checkpoint(output_dir)\n",
                "\n",
                "train_start = time.time()\n",
                "\n",
                "try:\n",
                "    if last_checkpoint is not None:\n",
                "        print(f'Resuming from checkpoint: {last_checkpoint}')\n",
                "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
                "    else:\n",
                "        print('Starting fresh Llama-2-13B QLoRA training...')\n",
                "        trainer.train()\n",
                "\n",
                "    train_elapsed = time.time() - train_start\n",
                "    print('=' * 60)\n",
                "    print(f'TRAINING COMPLETE — {train_elapsed / 60:.1f} minutes')\n",
                "    print('=' * 60)\n",
                "\n",
                "except torch.cuda.OutOfMemoryError:\n",
                "    print('\\n' + '=' * 60)\n",
                "    print('FATAL: CUDA Out of Memory')\n",
                "    print('=' * 60)\n",
                "    print(f'  Allocated: {torch.cuda.memory_allocated() / 1024**3:.1f} GB')\n",
                "    print(f'  Reserved:  {torch.cuda.memory_reserved() / 1024**3:.1f} GB')\n",
                "    print(f'  Total:     {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
                "    print('  Action:    Reduce per_device_train_batch_size to 8 and retry.')\n",
                "    print('=' * 60)\n",
                "    torch.cuda.empty_cache()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_model",
            "metadata": {},
            "outputs": [],
            "source": [
                "final_model_dir = '/content/drive/MyDrive/fineweb_edu_llama2_13b/final_model'\n",
                "print(f'Saving LoRA adapters to: {final_model_dir}')\n",
                "trainer.save_model(final_model_dir)\n",
                "tokenizer.save_pretrained(final_model_dir)\n",
                "print('Model saved successfully.')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "rag_md",
            "metadata": {},
            "source": [
                "## 5. Build RAG Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_build",
            "metadata": {},
            "outputs": [],
            "source": [
                "import faiss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np\n",
                "\n",
                "RAG_SAMPLES = 100_000\n",
                "RAG_DIR = '/content/drive/MyDrive/fineweb_edu_llama2_13b/rag_index'\n",
                "os.makedirs(RAG_DIR, exist_ok=True)\n",
                "\n",
                "passages = []\n",
                "rag_stream = raw_dataset.take(RAG_SAMPLES)\n",
                "\n",
                "print('Extracting passages...')\n",
                "for row in tqdm(rag_stream, total=RAG_SAMPLES):\n",
                "    text = row['text'].strip()\n",
                "    for i in range(0, len(text), 500):\n",
                "        chunk = text[i:i + 500].strip()\n",
                "        if len(chunk) > 50:\n",
                "            passages.append(chunk)\n",
                "\n",
                "print(f'Encoding {len(passages):,} passages...')\n",
                "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
                "embeddings = embedder.encode(passages, show_progress_bar=True, batch_size=256, convert_to_numpy=True)\n",
                "\n",
                "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
                "faiss.normalize_L2(embeddings)\n",
                "index.add(embeddings)\n",
                "\n",
                "faiss.write_index(index, os.path.join(RAG_DIR, 'faiss_index.bin'))\n",
                "np.save(os.path.join(RAG_DIR, 'passages.npy'), np.array(passages, dtype=object))\n",
                "print(f'RAG index saved: {len(passages):,} passages, dim={embeddings.shape[1]}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}