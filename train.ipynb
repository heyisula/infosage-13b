{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "colab_setup_md",
            "metadata": {},
            "source": [
                "# Colab Setup\n",
                "\n",
                "**Step 1: Install & Auto-Restart**\n",
                "Run the installation cell below. If it says \"RESTARTING RUNTIME\", let it finish, then start from **Step 2**.\n",
                "This is necessary for the GPU drivers to recognize the 4-bit quantization package."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ec19dd69",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'\n",
                "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "colab_install",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Force install the exact versions required for QLoRA\n",
                "# Note: Adafactor is now part of transformers/optimum or installed via source if unavailable on PyPI directly for some python versions\n",
                "# We'll install it separately if needed, but 'transformers' usually handles it.\n",
                "!pip install -q -U \"bitsandbytes>=0.46.1\" \"accelerate>=1.0.0\" peft transformers datasets tqdm faiss-cpu sentence-transformers\n",
                "\n",
                "import bitsandbytes\n",
                "import os\n",
                "from packaging import version\n",
                "\n",
                "required_version = \"0.46.1\"\n",
                "current_version = bitsandbytes.__version__\n",
                "\n",
                "if version.parse(current_version) < version.parse(required_version):\n",
                "    print(f\"\\n[!] bitsandbytes version {current_version} is too old. RESTARTING RUNTIME...\")\n",
                "    os.kill(os.getpid(), 9)\n",
                "else:\n",
                "    print(f\"\\n[OK] bitsandbytes {current_version} is ready!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "colab_drive",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Mount Google Drive to save the model permanently\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a1b2c3d4",
            "metadata": {},
            "source": [
                "# Fine-Tune GPT-2 Large on FineWeb-Edu (QLoRA Version)\n",
                "\n",
                "This notebook fine-tunes **GPT-2 Large (774M params)** using **QLoRA (4-bit Quantized LoRA)**.\n",
                "**Why QLoRA?** Standard GPT-2 Large training hits OOM on T4 (15GB). QLoRA reduces the base model footprint to ~500MB, allowing full training with 1024 context length comfortably."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "84a6e95e",
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    GPT2LMHeadModel,\n",
                "    GPT2TokenizerFast,\n",
                "    Trainer,\n",
                "    TrainingArguments,\n",
                "    DataCollatorForLanguageModeling,\n",
                "    BitsAndBytesConfig\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
                "from tqdm import tqdm"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "6c5c5849",
            "metadata": {},
            "outputs": [],
            "source": [
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {device}\")\n",
                "if device == \"cuda\":\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e5f6g7h8",
            "metadata": {},
            "source": [
                "## 1. Load Pre-trained GPT-2 Large Model in 4-bit"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b2307b3a",
            "metadata": {},
            "outputs": [],
            "source": [
                "MODEL_NAME = \"gpt2-large\"\n",
                "\n",
                "print(f\"Loading model {MODEL_NAME} in 4-bit...\")\n",
                "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "\n",
                "# 4-bit Quantization Config\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.float16\n",
                ")\n",
                "\n",
                "model = GPT2LMHeadModel.from_pretrained(\n",
                "    MODEL_NAME, \n",
                "    quantization_config=bnb_config,\n",
                "    device_map={ \"\": 0 } \n",
                ")\n",
                "\n",
                "model.gradient_checkpointing_enable()\n",
                "model = prepare_model_for_kbit_training(model)\n",
                "\n",
                "print(\"Applying LoRA configuration...\")\n",
                "peft_config = LoraConfig(\n",
                "    task_type=TaskType.CAUSAL_LM, \n",
                "    inference_mode=False, \n",
                "    r=16, \n",
                "    lora_alpha=32, \n",
                "    lora_dropout=0.05,\n",
                "    target_modules=[\"c_attn\"] \n",
                ")\n",
                "\n",
                "model = get_peft_model(model, peft_config)\n",
                "model.print_trainable_parameters()\n",
                "\n",
                "print(f\"Max context length: {model.config.n_positions}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "i9j0k1l2",
            "metadata": {},
            "source": [
                "## 2. Prepare Streaming Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "37a18622",
            "metadata": {},
            "outputs": [],
            "source": [
                "NUM_SAMPLES = 1_000_000\n",
                "MAX_LENGTH = 1024 \n",
                "\n",
                "print(f\"Configuring stream for {NUM_SAMPLES:,} samples...\")\n",
                "\n",
                "raw_dataset = load_dataset(\n",
                "    \"HuggingFaceFW/fineweb-edu\",\n",
                "    split=\"train\",\n",
                "    streaming=True\n",
                ")\n",
                "\n",
                "def tokenize_stream(examples):\n",
                "    tokenized = tokenizer(\n",
                "        examples[\"text\"],\n",
                "        truncation=True,\n",
                "        max_length=MAX_LENGTH,\n",
                "        padding=\"max_length\"\n",
                "    )\n",
                "    return {\n",
                "        \"input_ids\": tokenized[\"input_ids\"],\n",
                "        \"attention_mask\": tokenized[\"attention_mask\"]\n",
                "    }\n",
                "\n",
                "sample = next(iter(raw_dataset))\n",
                "all_columns = list(sample.keys())\n",
                "\n",
                "tokenized_dataset = raw_dataset.map(\n",
                "    tokenize_stream, \n",
                "    batched=True, \n",
                "    remove_columns=all_columns, \n",
                "    batch_size=1000\n",
                ")\n",
                "\n",
                "shuffled_dataset = tokenized_dataset.shuffle(seed=42, buffer_size=10_000).take(NUM_SAMPLES)\n",
                "print(\"Streaming dataset configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "q7r8s9t0",
            "metadata": {},
            "source": [
                "## 3. Configure Training with Drive Checkpoints"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "360e73d9",
            "metadata": {},
            "outputs": [],
            "source": [
                "output_dir = \"/content/drive/MyDrive/fineweb_edu_gpt2_large/checkpoints\"\n",
                "os.makedirs(output_dir, exist_ok=True)\n",
                "\n",
                "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
                "\n",
                "BATCH_SIZE = 1\n",
                "GRAD_ACCUM = 8\n",
                "TOTAL_STEPS = NUM_SAMPLES // (BATCH_SIZE * GRAD_ACCUM)\n",
                "\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=output_dir,\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM,\n",
                "    learning_rate=2e-4,\n",
                "    max_steps=TOTAL_STEPS,\n",
                "    fp16=True,             \n",
                "    gradient_checkpointing=True,\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    logging_steps=100,\n",
                "    save_steps=500,\n",
                "    save_total_limit=3,\n",
                "    report_to=\"none\",\n",
                "    remove_unused_columns=False\n",
                ")\n",
                "\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=shuffled_dataset,\n",
                "    data_collator=data_collator,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "u1v2w3x4",
            "metadata": {},
            "source": [
                "## 4. Train (Smart Auto-Resume)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "28100396",
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers.trainer_utils import get_last_checkpoint\n",
                "last_checkpoint = get_last_checkpoint(output_dir)\n",
                "\n",
                "if last_checkpoint is not None:\n",
                "    print(f\"Checkpoint detected: {last_checkpoint}. Resuming...\")\n",
                "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
                "else:\n",
                "    print(\"Starting fresh QLoRA training...\")\n",
                "    trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4fca0420",
            "metadata": {},
            "outputs": [],
            "source": [
                "final_model_dir = \"/content/drive/MyDrive/fineweb_edu_gpt2_large/final_model\"\n",
                "print(f\"Saving final LoRA adapters to: {final_model_dir}...\")\n",
                "trainer.save_model(final_model_dir)\n",
                "tokenizer.save_pretrained(final_model_dir)\n",
                "print(\"Training Complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c9d0e1f2",
            "metadata": {},
            "source": [
                "## 5. Build RAG Index"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "rag_build_02",
            "metadata": {},
            "outputs": [],
            "source": [
                "import faiss\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import numpy as np\n",
                "\n",
                "RAG_SAMPLES = 100_000\n",
                "RAG_DIR = \"/content/drive/MyDrive/fineweb_edu_gpt2_large/rag_index\"\n",
                "os.makedirs(RAG_DIR, exist_ok=True)\n",
                "\n",
                "passages = []\n",
                "rag_stream = raw_dataset.take(RAG_SAMPLES)\n",
                "\n",
                "print(\"Extracting passages from stream...\")\n",
                "for row in tqdm(rag_stream, total=RAG_SAMPLES):\n",
                "    text = row[\"text\"].strip()\n",
                "    for i in range(0, len(text), 500):\n",
                "        chunk = text[i:i + 500].strip()\n",
                "        if len(chunk) > 50: passages.append(chunk)\n",
                "\n",
                "embedder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "embeddings = embedder.encode(passages, show_progress_bar=True, batch_size=256, convert_to_numpy=True)\n",
                "\n",
                "index = faiss.IndexFlatIP(embeddings.shape[1])\n",
                "faiss.normalize_L2(embeddings)\n",
                "index.add(embeddings)\n",
                "\n",
                "faiss.write_index(index, os.path.join(RAG_DIR, \"faiss_index.bin\"))\n",
                "np.save(os.path.join(RAG_DIR, \"passages.npy\"), np.array(passages, dtype=object))\n",
                "print(\"RAG Index Built!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbformat_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}